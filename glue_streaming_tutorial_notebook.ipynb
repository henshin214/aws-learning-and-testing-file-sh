{
	"cells": [
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"# AWS Glue Streaming Tutorials - Working with Studio Notebook\n",
				"##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"### Optional: Run this cell to see available notebook commands (\"magics\").\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%help"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Start by setting the Glue Job Type to streaming "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%streaming"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"###  Run this cell to set up and start your interactive session.\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"%glue_version 3.0\n",
				"\n",
				"import sys\n",
				"from awsglue.transforms import *\n",
				"from awsglue.utils import getResolvedOptions\n",
				"from pyspark.context import SparkContext\n",
				"from awsglue.context import GlueContext\n",
				"from awsglue.job import Job\n",
				"from awsglue import DynamicFrame\n",
				"from datetime import datetime\n",
				"from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
				"from pyspark.sql.functions import lit,col,from_json, split\n",
				"import boto3\n",
				"  \n",
				"sc = SparkContext.getOrCreate()\n",
				"glueContext = GlueContext(sc)\n",
				"spark = glueContext.spark_session\n",
				"job = Job(glueContext)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"### Define the variables to be used - please replace with relevant values \n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"output_database_name=\"REPLACE_MA\"\n",
				"output_table_name=\"REPLACE_ME\"\n",
				"\n",
				"account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
				"region_name=boto3.client('s3').meta.region_name\n",
				"stream_arn_name = \"arn:aws:kinesis:{}:{}:stream/GlueStreamTest-{}\".format(region_name,account_id,account_id)\n",
				"s3_bucket_name = \"streaming-tutorial-s3-target-{}\".format(account_id)\n",
				"\n",
				"output_location = \"s3://{}/streaming_output/\".format(s3_bucket_name)\n",
				"checkpoint_location = \"s3://{}/checkpoint_location/\".format(s3_bucket_name)\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Read the events from kinesis data streams\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"data_frame = glueContext.create_data_frame.from_options(\n",
				"    connection_type=\"kinesis\",\n",
				"    connection_options={\n",
				"        \"typeOfData\": \"kinesis\",\n",
				"        \"streamARN\": stream_arn_name,\n",
				"        \"classification\": \"json\",\n",
				"        \"startingPosition\": \"earliest\",\n",
				"        \"inferSchema\": \"true\",\n",
				"    },\n",
				"    transformation_ctx=\"data_frame\",\n",
				")\n"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Sample and print the incoming records\n",
				"#### the sampling is for debugging purpose. You may comment off the entire code cell below, before deploying the actual code "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"options = {\n",
				"\t\"pollingTimeInMs\": \"20000\",\n",
				"\t\"windowSize\": \"5 seconds\"\n",
				"}\n",
				"sampled_dynamic_frame = glueContext.getSampleStreamingDynamicFrame(data_frame, options, None)\n",
				"\n",
				"count_of_sampled_records = sampled_dynamic_frame.count()\n",
				"\n",
				"print(count_of_sampled_records)\n",
				"\n",
				"sampled_dynamic_frame.printSchema()\n",
				"\n",
				"sampled_dynamic_frame.toDF().show(10,False)\n",
				"  "
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"source": [
				"### Define the process batch method which will be executed in every microbatch. \n",
				"#### The business logic is defined in this method. "
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"def processBatch(data_frame, batchId):\n",
				"    if data_frame.count() > 0:\n",
				"        \n",
				"        schema = StructType([StructField(\"eventtime\", StringType()),\n",
				"                             StructField(\"manufacturer\", StringType()),\n",
				"                             StructField(\"minutevolume\", LongType()),\n",
				"                             StructField(\"o2stats\", LongType()),\n",
				"                             StructField(\"pressurecontrol\", LongType()),\n",
				"                             StructField(\"serialnumber\", StringType()),\n",
				"                             StructField(\"ventilatorid\", LongType())])\n",
				"        \n",
				"        data_frame = data_frame.select(from_json(col(\"$json$data_infer_schema$_temporary$\").cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
				"        \n",
				"        '''\n",
				"        As a part of the transformation, in this block, we will \n",
				"        1/ select only a few columns from the incoming stream [eventtime,manufacturer,o2stats,serialnumber,ventilatorid]\n",
				"        2/ rename column o2stats to oxygen_stats\n",
				"        3/ derive a few new columns [serial_identifier,ingest_year,ingest_month,ingest_day]\n",
				"        4/ store the results into an s3 bucket and also a catalog table, partioned by the derived columns\n",
				"        '''\n",
				"        \n",
				"        data_frame = data_frame.select(\"eventtime\", \"manufacturer\",\"o2stats\",\"serialnumber\",\"ventilatorid\")\n",
				"        \n",
				"        data_frame = data_frame.withColumnRenamed(\"o2stats\", \"oxygen_stats\")\n",
				"        \n",
				"        \n",
				"         \n",
				"        current_datetime_utc = datetime.utcnow()\n",
				"        current_year = str(current_datetime_utc.year)\n",
				"        current_month = str(current_datetime_utc.month)\n",
				"        current_day = str(current_datetime_utc.day)\n",
				"        \n",
				"        data_frame = data_frame.withColumn(\"ingest_year\", lit(current_year)).withColumn(\"ingest_month\", lit(current_month)).withColumn(\"ingest_day\", lit(current_day))\n",
				"        data_frame = data_frame.withColumn('serial_identifier', split(data_frame['serialnumber'], '-').getItem(0))\n",
				"        \n",
				"        kinesis_dynamic_frame = DynamicFrame.fromDF(data_frame, glueContext, \"kinesis_dynamic_frame\")\n",
				"        \n",
				"        amazon_s3_node = glueContext.getSink(\n",
				"            path=output_location,\n",
				"            connection_type=\"s3\",\n",
				"            updateBehavior=\"UPDATE_IN_DATABASE\",\n",
				"            partitionKeys=[\"ingest_year\", \"ingest_month\", \"ingest_day\"],\n",
				"            enableUpdateCatalog=True,\n",
				"            transformation_ctx=\"amazon_s3_dyf\",\n",
				"        )\n",
				"        amazon_s3_node.setCatalogInfo(\n",
				"            catalogDatabase=output_database_name, catalogTableName=output_table_name\n",
				"        )\n",
				"        amazon_s3_node.setFormat(\"glueparquet\")\n",
				"        amazon_s3_node.writeFrame(kinesis_dynamic_frame)"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"### Trigger the microbatched execution by calling processBatch"
			]
		},
		{
			"cell_type": "code",
			"execution_count": null,
			"metadata": {
				"editable": true,
				"trusted": true,
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"outputs": [],
			"source": [
				"glueContext.forEachBatch(\n",
				"    frame=data_frame,\n",
				"    batch_function=processBatch,\n",
				"    options={\n",
				"        \"windowSize\": \"10 seconds\",\n",
				"        \"checkpointLocation\": checkpoint_location\n",
				"    },\n",
				")"
			]
		},
		{
			"cell_type": "markdown",
			"metadata": {},
			"source": [
				"##### If everything went well, you would't see any output/print statements from the above cell. You can inspect the S3 location or Athena table for the data "
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 4
}
